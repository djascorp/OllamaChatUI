# Ollama Next.js Chat UI

![Langage](https://img.shields.io/badge/language-TypeScript-blue.svg)
![Framework](https://img.shields.io/badge/framework-Next.js-black.svg)
![Style](https://img.shields.io/badge/style-TailwindCSS-38B2AC.svg)
![Licence](https://img.shields.io/badge/license-MIT-green.svg)

Une interface de chat web complÃ¨te, moderne et open-source pour interagir avec vos grands modÃ¨les de langage (LLM) locaux via **Ollama**. Profitez d'une expÃ©rience de chat privÃ©e, rapide et fluide, directement dans votre navigateur.

![AperÃ§u de l'application Ollama Chat UI](https://raw.githubusercontent.com/ollama/ollama-webui/main/screenshot.png)
*(Note: L'image ci-dessus est un exemple d'interface ; remplacez-la par une capture d'Ã©cran de votre application une fois terminÃ©e.)*

## âœ¨ FonctionnalitÃ©s

*   **ğŸ¤– Chat en temps rÃ©el :** Discutez avec n'importe quel modÃ¨le supportÃ© par Ollama.
*   **âš¡ï¸ RÃ©ponses en Streaming :** Les rÃ©ponses de l'IA s'affichent mot par mot pour une expÃ©rience utilisateur instantanÃ©e.
*   **ğŸ§  SÃ©lection de ModÃ¨les :** Changez de modÃ¨le Ã  la volÃ©e grÃ¢ce Ã  un menu dÃ©roulant qui liste automatiquement vos modÃ¨les locaux.
*   **ğŸ”’ 100% PrivÃ© :** Toutes les requÃªtes sont traitÃ©es entre votre navigateur et votre serveur Ollama local. Aucune donnÃ©e ne quitte votre machine.
*   **ğŸ“ Support du Markdown :** Les rÃ©ponses sont formatÃ©es avec le support complet du Markdown, y compris les blocs de code avec coloration syntaxique et un bouton "Copier".
*   **ğŸš€ Architecture Moderne :** Construit avec le **Next.js App Router**, garantissant des performances optimales et une expÃ©rience de dÃ©veloppement moderne.
*   **ğŸ¨ UI Ã‰lÃ©gante :** Interface Ã©purÃ©e et responsive construite avec **Tailwind CSS** et **shadcn/ui**.

## ğŸ› ï¸ Stack Technique

*   **Framework :** [Next.js](https://nextjs.org/) (App Router)
*   **Langage :** [TypeScript](https://www.typescriptlang.org/)
*   **Style :** [Tailwind CSS](https://tailwindcss.com/) & [shadcn/ui](https://ui.shadcn.com/)
*   **IA Backend :** [Ollama](https://ollama.com/)
*   **Rendu Markdown :** `react-markdown` & `react-syntax-highlighter`

## ğŸ“‹ PrÃ©requis

Avant de commencer, assurez-vous d'avoir les Ã©lÃ©ments suivants installÃ©s sur votre machine :

1.  **[Node.js](https://nodejs.org/en/)** (version 18.0 ou supÃ©rieure)
2.  **[Ollama](https://ollama.com/)** installÃ© et en cours d'exÃ©cution.

Pour vÃ©rifier qu'Ollama fonctionne, ouvrez votre terminal et exÃ©cutez :
```bash
ollama --version
```

Assurez-vous Ã©galement d'avoir tÃ©lÃ©chargÃ© au moins un modÃ¨le. Par exemple, pour tÃ©lÃ©charger Llama 3 :
```bash
ollama run llama3
```

## ğŸš€ DÃ©marrage Rapide

Suivez ces Ã©tapes pour lancer l'application localement :

1.  **Clonez le dÃ©pÃ´t :**
    ```bash
    git clone https://github.com/djascorp/OllamaChatUI.git
    cd OllamaChatUI
    ```

2.  **Installez les dÃ©pendances :**
    ```bash
    npm install
    # ou yarn install, ou pnpm install
    ```

3.  **Configurez les variables d'environnement :**
    CrÃ©ez un fichier `.env.local` Ã  la racine du projet en copiant le fichier `.env.example` :
    ```bash
    cp .env.example .env.local
    ```
    Le fichier `.env.local` contiendra l'URL de base de votre serveur Ollama. La valeur par dÃ©faut est gÃ©nÃ©ralement correcte.
    ```env
    # .env.local
    OLLAMA_API_BASE_URL=http://localhost:11434
    ```

4.  **Lancez le serveur de dÃ©veloppement :**
    ```bash
    npm run dev
    ```

5.  **Ouvrez votre navigateur :**
    Rendez-vous sur [http://localhost:3000](http://localhost:3000) et commencez Ã  discuter avec vos modÃ¨les locaux !

## ğŸ—ï¸ Structure du Projet

Voici un aperÃ§u des fichiers et dossiers importants du projet :

```
/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ layout.tsx         # Layout principal de l'application
â”‚   â”œâ”€â”€ page.tsx           # Page d'accueil qui rend le client de chat
â”‚   â””â”€â”€ api/
â”‚       â””â”€â”€ chat/
â”‚           â””â”€â”€ route.ts   # API Route (proxy) qui communique avec Ollama
â”‚
â”œâ”€â”€ components/
â”‚   â””â”€â”€ ChatClient.tsx     # Le composant principal gÃ©rant l'UI et la logique du chat
â”‚
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ utils.ts           # Fonctions utilitaires (ex: pour shadcn/ui)
â”‚   â””â”€â”€ types.ts           # DÃ©finitions des types TypeScript
â”‚
â”œâ”€â”€ .env.local             # Vos variables d'environnement locales (non versionnÃ©es)
â”œâ”€â”€ next.config.mjs        # Configuration de Next.js
â””â”€â”€ package.json           # DÃ©pendances et scripts du projet
```

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Si vous souhaitez amÃ©liorer ce projet, n'hÃ©sitez pas Ã  :
1.  Forker le dÃ©pÃ´t.
2.  CrÃ©er une nouvelle branche (`git checkout -b feature/nouvelle-fonctionnalite`).
3.  Faire vos modifications.
4.  Soumettre une Pull Request.

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.